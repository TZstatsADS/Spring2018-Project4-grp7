samp_size = c(5, 10, 25, 50, 100)
n = 500
samp_means = cbind(rep(NA, n*length(samp_size)),rep(NA, n*length(samp_size)))
colnames(samp_means) = c("Values", "SampleSize")
for (j in 1:length(samp_size)) {
for (i in 1:n) {
row = (j - 1)*n + i
samp_means[row,1] = mean(rnorm(samp_size[j], 10, 3))
samp_means[row,2] = samp_size[j]
}
}
ggplot(data.frame(samp_means)) +
geom_histogram(aes(x = Values, y = ..density..)) +
geom_density(aes(x = Values)) +
facet_wrap(~SampleSize)
gam.MMest = function(data) {
m = mean(data)
v = var(data)
return(c(a = m^2/v, s = v/m))
}
cat.MM = gam.MMest(cats$Hwt)
gam_args = list(shape = cat.MM["a"], scale = cat.MM["s"])
ggplot(cats) +
geom_histogram(aes(x = Hwt, y = ..density..)) +
geom_density(aes(x = Hwt), linetype = "dashed") +
stat_function(aes(x = Hwt), fun = dgamma, args = gam_args, color = "red")
gam.mean = function(a,s) {return(a*s)}
gam.var = function(a,s) {return(a*s^2)}
gam.diff = function(params, data) {
a = params[1]
s = params[2]
return((mean(data) - gam.mean(a,s))^2 + (var(data) - gam.var(a,s))^2)
}
nlm(gam.diff, c(19,1), data = cats$Hwt)[1:3]
gam.MMest(rgamma(100, shape = 19, scale = 45))
gam.MMest(rgamma(1000, shape = 19, scale = 45))
gam.MMest(rgamma(10000, shape = 19, scale = 45))
gam.ll = function(params,data) {
a = params[1]
s = params[2]
return(sum(dgamma(data, shape = a, scale = s, log = T)))
}
gam.ll(params = c(19, 0.5), data = cats$Hwt)
nlm(gam.ll, c(19,1), data = cats$Hwt)$est
neg.gam.ll = function(params, data){
a = params[1]
s = params[2]
return(-sum(dgamma(data, shape = a, scale = s, log = T)))
}
nlm(neg.gam.ll, c(19,1), data = cats$Hwt)$minimum
nlm(neg.gam.ll, c(19,1), data = cats$Hwt)$estimate
cat.MM = gam.MMest(cats$Hwt)
neg.gam.ll(cat.MM,cats$Hwt)
cat.MLE = nlm(neg.gam.ll, c(19,1), data = cats$Hwt)$estimate
MM_args = list(shape = cat.MM["a"], scale = cat.MM["s"])
MLE_args = list(shape = cat.MLE[1], scale = cat.MLE[2])
ggplot(cats) +
geom_histogram(aes(x = Hwt, y = ..density..)) +
geom_density(aes(x = Hwt), linetype = "dashed") +
stat_function(aes(x = Hwt), fun = dgamma, args = MM_args, color = 2) +
stat_function(aes(x = Hwt), fun = dgamma, args = MLE_args, color = 4)
qgamma(c(0.01,0.05,0.95,0.99), shape = cat.MM["a"], scale = cat.MM["s"])
quantile(cats$Hwt, c(0.01,0.05,0.95,0.99))
a = cat.MM["a"]
s = cat.MM["s"]
qqplot(cats$Hwt, qgamma((1:99)/100, shape = a, scale = s),
ylab ="Theoretical Quantiles")
abline(intercept = 0, slope = 1, col = "red")
plot(ecdf(pgamma(cats$Hwt, shape = a, scale = s)),
main = "Calibration of dgam for cat hearts")
abline(0,1,col = "red")
ks.test(cats$Hwt, pgamma, shape = a, scale = s)
n = length(cats$Hwt)
train = sample(1:n, size = round(.9*n))
cat.MM = gam.MMest(cats$Hwt[train])
a = cat.MM["a"]
s = cat.MM["s"]
c(a,s)
ks.test(cats$Hwt[-train], pgamma, shape = a, scale = s)
ks.test(cats$Hwt[cats$Sex == "F"], cats$Hwt[cats$Sex == "M"])
cat.MM
n = nrow(cats)
resamp = sample(1:n, n, replace = T)
head(sort(cats$Hwt))
head(sort(cats$Hwt[resamp]))
gam.MMest(cats$Hwt[resamp])
B = 1000
param_ests = matrix(NA, nrow = B, ncol = 2)
colnames(param_ests) = c("a", "s")
for (b in 1:B){
resamp = sample(1:n, n, replace = T)
param_ests[b, ] = gam.MMest(cats$Hwt[resamp])
}
head(param_ests)
param_ests = data.frame(param_ests)
ggplot(param_ests) +
geom_histogram(aes(x = a)) +
geom_vline(xintercept = mean(a), col = 2)
ggplot(param_ests) +
geom_histogram(aes(x = s)) +
geom_vline(xintercept = mean(s), col = 2)
(bootstrap.a = mean(param_ests$a))
(bootstrap.s = mean(param_ests$s))
(CI.a = quantile(param_ests$a, probs = c(0.025, 0.975)))
(CI.s = quantile(param_ests$s, probs = c(0.025, 0.975)))
ggplot(param_ests) +
geom_histogram(aes(x = a)) +
geom_vline(xintercept = mean(a), col = 2) +
geom_vline(xintercept = CI.a[1], col = 2, linetype = "dashed")+
geom_vline(xintercept = CI.a[2], col = 2, linetype = "dashed")
ggplot(param_ests) +
geom_histogram(aes(x = a)) +
geom_vline(xintercept = mean(a), col = 2) +
geom_vline(xintercept = CI.a[1], col = 2, linetype = "dashed")+
geom_vline(xintercept = CI.a[2], col = 2, linetype = "dashed")
set.seed(1)
samp_size = 100
samp1 = rnorm(samp_size, mean = 10, sd = 3)
mmean_est1 = mean(samp1)
n <- length(samp1)
B <- 1000
boot.means <- rep(NA, B)
for (b in 1:B) {
resamp_data   <- sample(samp1, n, replace = TRUE)
boot.means[b] <- mean(resamp_data)
}
mean(boot.means)
sd(boot.means)
ggplot() +
geom_histogram(aes(x = boot.means))
ggplot(cats) +
geom_boxplot(aes(x = Sex, y = Hwt)) +
labs(title = "Male and Female Cat Heart Weight")
ggplot() +
geom_histogram(aes(x = boot.means))
ggplot(cats) +
geom_boxplot(aes(x = Sex, y = Hwt)) +
labs(title = "Male and Female Cat Heart Weight")
girlcats = cats$Sex == "F"
t.test(cats$Hwt[girlcats], cats$Hwt[!girlcats])
girlcats = cats$Sex == "F"
Dhat = mean(cats$Hwt[girlcats] - mean(cats$Hwt[!girlcats]))
nf = sum(girlcats)
nm = sum(!girlcats)
P = 1000000
sample_diffs = rep(NA, P)
girlcats = cats$Sex == "F"
t.test(cats$Hwt[girlcats], cats$Hwt[!girlcats])
girlcats = cats$Sex == "F"
Dhat = mean(cats$Hwt[girlcats] - mean(cats$Hwt[!girlcats]))
nf = sum(girlcats)
nm = sum(!girlcats)
P = 1000000
sample_diffs = rep(NA, P)
for (i in 1:P) {
girl.cat = sample(c(1:(nf + nm)), nf)
sum.cat.girl = sum(cats$Hwt[girl.cat])
sum.cat.boy = sum(cats$Hwt) - sum.cat.girl
sample_diffs[i] = sum.cat.girl/nf - sum.cat.boy/nm
}
pval = mean(abs(sample_diffs) >= abs(Dhat))
pval
data(cats, package = "MASS")
head(cats)
head(cats$Hwt[cats$Sex == "M"])
head(cats[cats$Sex == "M", "Hwt"])
cats.subset = sample(1:nrow(cats), size = nrow(cats)/2)
head(cats.subset)
new.cats = cats[cats.subset,]
head(new.cats, 3)
males = cats$Sex == "M"
row.ind = sample(1:nrow(cats), size = nrow(cats)/2)
head(row.ind, 5)
males = cats$Sex == "M"
row.ind = sample(1:nrow(cats), size = nrow(cats)/2)
head(row.ind, 5)
boy.cats.1 = subset(cats, Sex == "M")
boy.cats.2 = cats[cats$Sex == "M",]
all(boy.cats.1 == boy.cats.2)
boy.cats.1 = subset(cats, Sex == "M")
boy.cats.2 = cats[cats$Sex == "M",]
all(boy.cats.1 == boy.cats.2)
states = data.frame(state.x77, Region = state.region)
head(states, 3)
states$Income[states$Region == "South"]
states[states$Region == "South", "Income"]
dim(is.na(cats))
sum(states$Frost >= 150)
row.names(states)[states$Frost >= 150]
avgs = colSums(states[, 1:8])/nrow(states)
colSums(states[, 1:8] > rep(avgs, each = nrow(states)))
(mat1 = states[, 1:8] > matrix(rep(avgs, each = nrow(states)), ncol = 8))
(mat2 = states[, 1:8] > avgs)
(mat1 = states[, 1:8] > matrix(rep(avgs, each = nrow(states)), ncol = 8))
(mat2 = states[, 1:8] > avgs)
apply(states[, 1:8],2,max)
apply(states[, 1:8],2,which.max)
rownames(states)[apply(states[,1:8],2,which.max)]
apply(states[,1:5],2,summary)
frow = function(r) {
val = as.numeric(r[7])
return(ifelse(r[9] == "Northeast", 0.5*val, val))
}
frost.fake = apply(states, 1, frow)
mean(states$Frost[states$Region == "Northeast"])
mean(frost.fake[states$Region == "Northeast"])
top.3.names = function(v, names.v) {
names.v[order(v,decreasing = T)[1:3]]
}
apply(states[, 1:7],2,top.3.names, names.v = rownames(states))
cor.v1.v2 <- function(v1, v2 = states[, "Frost"]) {
return(cor(v1, v2))
}
cor.v1.v2(states$Life.Exp)
cor.v1.v2(states$Frost)
apply(states[, 1:8], 2, cor.v1.v2)
apply(states[, 1:8], 2, cor, states[, "Frost"])
cor.v1.v2(states$Life.Exp)
cor.v1.v2(states$Frost)
apply(states[, 1:8], 2, cor.v1.v2)
apply(states[, 1:8], 2, cor, states[, "Frost"])
mean.less.one = function(i, vec) {
return(mean(vec[-i]))
}
my.vec = states[,"Frost"]
n = length(my.vec)
my.vec.jack = lapply(1:n, mean.less.one, my.vec)
head(my.vec.jack, 3)
my.vec.jack = sapply(1:n, mean.less.one, my.vec)
head(my.vec.jack, 3)
tapply(states[,"Frost"], state.region, mean)
mapply(rep, 1:4, 4:1)
my.vec.jack = lapply(1:n, mean.less.one, my.vec)
head(my.vec.jack, 3)
my.vec.jack = sapply(1:n, mean.less.one, my.vec)
head(my.vec.jack, 3)
tapply(states[,"Frost"], state.region, mean)
mapply(rep, 1:4, 4:1)
head(cats, 3)
hwt.order = order(cats$Hwt)
cats.order = cats[hwt.order, ]
this.vec = c(25,13,25,77,68)
head(cats.order,3)
this.vec[order(this.vec)]
sort(this.vec)
which.min(cats$Hwt) == order(cats$Hwt)[1]
knitr::opts_chunk$set(echo = TRUE)
pbinom(16, size = 25, prob = 0.5)
1 - pbinom(16, size = 25, prob = 0.5)
1 - pbin(16, size = 25, prob = 0.5)
1 - pbinom(16, size = 25, prob = 0.5)
p = pnorm(0,0.5, 1)
p
p = 1 - pnorm(0,0.5, 1)
p
p = 1 - pnorm(0,0.5,1)
pbinom(16,1,p)
p = 1 - pnorm(0,0.5,1)
pbinom(16,1,p)
1 - pbinom(1.4, size = 25, prob = 0.5)
pbinom(1.4, size = 25, prob = 0.5)
1 - pbinom(1.4, size = 25, prob = 0.5)
pnorm(1.4)
1 - pnorm(1.4)
1 - pbinom(16, 25, 0.5)
?pbinom
pnorm(1.6)
1 - pnorm(1.6)
1 - pnorm(1.6)
1 - pbinom(16, 0.5, 1)
1 - pnorm(1.6)
1 - pbinom(16, 25, 0.5)
1 - pnorm(1.6)
(1 - pbinom(16, 25, 0.5))*2
p = 1 - pnorm(0,0.5,1)
pbinom(16,1,p)
1 - pnorm(0,0.5,1)
pbinom(16,1,p)
mean(10, -2, -1, -4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3)
mean(10, -2, -1, -4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3)
var(10, -2, -1, -4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3)
summary(10, -2, -1, -4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3)
summary(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3)
summary(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
mean(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
mean(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
var(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
mean(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
sd(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
mean(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
var(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
mean(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
sd(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
qf(0.9975,1,19)
mean(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
sd(c(10,-2,-1,-4, 4, 5, 3, -3, -5,-5,-2,7,-3,-3,2,-7,-2,-4,-1,-3))
qf(0.025,1,19)
pretest  = c(30,28,31,26,20,30,34,15,28,20,30,29,31,29,34,20,26,25,31,29)
posttest = c(20,30,32,30,16,25,31,18,33,25,32,22,34,32,32,27,28,29,32,32)
difference = pretest - posttest
t.test(difference, mu = 0)
qqnorm(difference, main = "Q-Q plot")
abline(0,1, col = "red")
qqnorm(difference, main = "Q-Q plot")
qqline(difference, col = "red")
install.packages(BSDA)
install.packages("BSDA")
library(BSDA)
income = c(7, 1110, 7.1, 5.2, 8, 12, 0, 5, 2.1, 2, 46, 7.5)
summary(income)
t.test(income, mu = 1)
SIGN.test(income, md = 1, alternative = "two.sided")
2 * (1 - pbinom(10, 12,0.5))
age = c(54, 42, 51, 54, 49, 56, 33, 58, 54, 64, 49)
sort(age)
age = c(54, 42, 51, 54, 49, 56, 33, 58, 54, 64, 49)
sort(age)
summary(age)
library(BSDA)
income = c(7, 1110, 7.1, 5.2, 8, 12, 0, 5, 2.1, 2, 46, 7.5)
summary(income)
t.test(income, mu = 1)
SIGN.test(income, md = 1, alternative = "two.sided")
2 * (1 - pbinom(10, 12,0.5))
age = c(54, 42, 51, 54, 49, 56, 33, 58, 54, 64, 49)
sort(age)
summary(age)
t.test(age, mu = 50)
SIGN.test(age, md = 50)
x = c(20,18,23,5,14,8,18,22)
wilcox.test(x, mu = 10,conf.int = T, correct = F)
a = c(0.7, -1.6, -0.2, -1.2, 0.1, 3.4, 3.7, 0.8, 0.0, 2.0)
b = c(1.9, 0.8, 1.1, 0.1, -0.1, 4.4, 5.5, 1.6, 4.6, 3.0)
d = a - b
sleep = data.frame(a, b, d)
shapiro.test(sleep$d)
wilcox.test(d, mu = 0, conf.int = T)
response = c(47,43,62,68,41,39,46,40,67,71,42,46)
A = c(rep(c(rep("A1", 2),rep("A2", 2),rep("A3", 2)), 2))
B = c(rep("B1", 6), rep("B2", 6))
boxplot(response~A*B)
tapply(response, list(A), mean)
tapply(response, list(B), mean)
interaction.plot(A,B,response)
summary(aov(response~A*B))
summary(aov(response~A+B))
fit = aov(response~A)
fit = aov(response~A)
tk = TukeyHSD(fit, "A")
plot(tk)
tk
shiny::runApp('C:/Users/Jiang/Desktop')
shiny::runApp('GitHub/Spring2018-Project2-Group7/app')
install.packages("ploty")
install.packages("plotly")
runApp('GitHub/Spring2018-Project2-Group7/app')
runApp('GitHub/Spring2018-Project2-Group7/app')
runApp('GitHub/Spring2018-Project2-Group7/app')
shiny::runApp('GitHub/Spring2018-Project2-Group7/app')
runApp('GitHub/Spring2018-Project2-Group7/app')
runApp('GitHub/Spring2018-Project2-Group7/app')
runApp('GitHub/Spring2018-Project2-Group7/app')
runApp('GitHub/Spring2018-Project2-Group7/app')
runApp('GitHub/Spring2018-Project2-Group7/app')
shiny::runApp('GitHub/Spring2018-Project2-Group7/app')
runApp('GitHub/Spring2018-Project2-Group7/app')
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
setwd("./ads_spr2017_proj3")
experiment_dir <- "../data/zipcode/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep=""),
header=F)
setwd("C:/Users/Jiang/Desktop/Statistical ML/hw3")
options(warn = -1)
library(e1071)
library(ggplot2)
data5 = read.csv("train.5.txt",header = F)
data6 = read.csv("train.6.txt",header = F)
data5$label = -1
data6$label = 1
data = rbind(data5,data6)
data$label = as.factor(data$label)
set.seed(3)
test = sample(1:nrow(data),0.2*nrow(data),replace = F)
train = setdiff(1:nrow(data),test)
test  = data[test,]
train = data[train,]
# cross validation on linear SVM
tuned_linear = tune.svm(label~., data = train,cost = c(10^(-4:4)),kernel = "linear",
tunecontrol = tune.control(cross = 10))
setwd("C:/Users/Jiang/Desktop/Statistical ML/hw3")
options(warn = -1)
library(e1071)
library(ggplot2)
data5 = read.csv("train.5.txt",header = F)
data6 = read.csv("train.6.txt",header = F)
data5$label = -1
data6$label = 1
data = rbind(data5,data6)
data$label = as.factor(data$label)
set.seed(3)
test = sample(1:nrow(data),0.2*nrow(data),replace = F)
train = setdiff(1:nrow(data),test)
test  = data[test,]
train = data[train,]
# cross validation on linear SVM
tuned_linear = tune.svm(label~., data = train,cost = c(10^(-4:4)),kernel = "linear",
tunecontrol = tune.control(cross = 10))
summary(tuned_linear)
best_par_linear = tuned_linear$best.parameters
# cross validation on RBF kernel SVM
tuned_RBF = tune.svm(label~., data = train,gamma = 10^(-4:4), cost = c(10^(-2:6)),
kernel = "radial", tunecontrol = tune.control(cross = 10))
summary(tuned_RBF)
best_par_RBF = tuned_RBF$best.parameters
library(ggplot2)
ggplot(data = tuned_linear$performances,
mapping = aes(x = log10(cost), y = error)) +
geom_point() +
geom_line() +
ylab("error")+
xlab("Log of cost") +
ggtitle("Misclassification Rate with Different Cost")
library(ggplot2)
ggplot(data = tuned_RBF$performances,
mapping = aes(x = log10(cost),y = log10(gamma)))+
geom_tile(aes(fill = error),color = "white") +
scale_fill_gradient(low = "white",high = "steelblue") +
ylab("Logarithm of gamma") +
xlab("Logarithm of cost") +
labs(fill = "error") +
ggtitle("Misclassification Rate with Different Cost and Gamma")
options(warn = -1)
library(e1071)
library(knitr)
model.linear = svm(label~.,data = train, cost = best_par_linear, kernel = "linear")
pred.linear = predict(model.linear,test[,-ncol(test)])
error1 = mean(pred.linear!=test$label)
model.RBF = svm(label~., data = train, gamma = best_par.RBF[1],
cost = best_par.RBF[2], kernel = "radial")
options(warn = -1)
library(e1071)
library(knitr)
model.linear = svm(label~.,data = train, cost = best_par_linear, kernel = "linear")
pred.linear = predict(model.linear,test[,-ncol(test)])
error1 = mean(pred.linear!=test$label)
model.RBF = svm(label~., data = train, gamma = best_par_RBF[1],
cost = best_par.RBF[2], kernel = "radial")
options(warn = -1)
library(e1071)
library(knitr)
model.linear = svm(label~.,data = train, cost = best_par_linear, kernel = "linear")
pred.linear = predict(model.linear,test[,-ncol(test)])
error1 = mean(pred.linear!=test$label)
model.RBF = svm(label~., data = train, gamma = best_par_RBF[1],
cost = best_par_RBF[2], kernel = "radial")
pred.RBF = predict(model.RBF ,test[,-ncol(test)])
error2 = mean(pred.RBF!=test$label)
showtable = data.frame("Kernel" = c("Linear","RBF"),
"Misclassification Rate" = c(paste0(error1*100,"%"), paste0(error2*100,"%")))
kable(showtable, caption = "Misclassification Report")
print(paste0("The best cost parameter in linear SVM is ",best_par_linear,"."))
print(paste0("The best gamma parameter in RBF SVM is ",best_par_RBF[1],", and the best cost parameter is ",best_par_RBF[2],"."))
install.packages("pROC")
?pROC
library(pROC)
test <- read.csv("../../data/movie_test.csv")
getwd()
test <- read.csv("../data/movie_test.csv")
load("../output/memory_based_predict/movie/movie_pearson_threshold.Rdata")
setwd("~/GitHub/Spring2018-Project4-grp7/output/memory_based_predict/movie")
setwd("~/GitHub/Spring2018-Project4-grp7/output/memory_based_predict")
test <- read.csv("../../data/movie_test.csv")
load("movie_pearson_threshold.Rdata")
setwd("~/GitHub/Spring2018-Project4-grp7/output/memory_based_predict/movie")
setwd("~/GitHub/Spring2018-Project4-grp7/output/memory_based_predict/movie")
load("movie_pearson_threshold.Rdata")
pred_pear <- movie_pearson_threshold
load("movie_vector_threshold.Rdata")
pred_vec <- movie_vector_threshold
mae <- function(pred, test){
mae <- mean(abs(pred - test), na.rm = T)
return(mae)
}
test <- test[,-1]
mae(pred_pear, as.matrix(test))
mae(pred_vec, as.matrix(test))
## ROC
library("pROC")
roc_mat <- matrix(4,nrow = nrow(pred_pear),ncol = ncol(pred_pear))
#pred_pear_v <- as.vector(((pred_pear>=roc_mat) == (test >= roc_mat)))
#pred_pear_v <- pred_pear_v[is.na(pred_pear_v)]
pred_pear_v <- as.vector(pred_pear)
test_v <- as.vector(as.matrix(test))
idx1 <- which(is.na(pred_pear_v)==T)
idx2 <- which(is.na(test_v)==T)
idx <- union(idx1,idx2)
pred_pear_v <- pred_pear_v[setdiff(1:length(test_v),idx)]
test_v <- test_v[setdiff(1:length(test_v),idx)]
roc_pear <- roc(test_v>=4,as.numeric(pred_pear_v>=4))
plot(roc_pear)
roc_pear$auc
roc_pear
